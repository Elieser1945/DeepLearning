{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQcxnTr+fcRsDiQ2ZM5UrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elieser1945/DeepLearning/blob/main/Week_2_Classification_Coverdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 **Model MLP Dengan PyTorch dan TensorFlow**"
      ],
      "metadata": {
        "id": "4hG90QU1jqtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Model PyTorch"
      ],
      "metadata": {
        "id": "AlC1g-4wkG9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#import libraries"
      ],
      "metadata": {
        "id": "T8vYsYshirbE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ugF8l3G8XhIV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "QE_lWAWQi44A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/compressed_data.csv.gz\")"
      ],
      "metadata": {
        "id": "QObqeohZX6v4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membuat Dataset untuk Regresi"
      ],
      "metadata": {
        "id": "nfefxU-Hi2VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus baris dengan NaN di target\n",
        "df = df.dropna(subset=['Cover_Type'])\n",
        "\n",
        "\n",
        "# Pisahkan fitur (X) dan target (y)\n",
        "X = df.drop(columns=[\"Cover_Type\"])\n",
        "y = df[\"Cover_Type\"] - 1  # Mengurangi 1 agar kelas mulai dari 0 (karena PyTorch membutuhkan label 0-based)\n",
        "\n",
        "\n",
        "# Bagi dataset menjadi data latih dan uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# Normalisasi fitur\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Konversi ke Tensor PyTorch\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_torch = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "mYBvRG81ZCpd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membangun Model MLP"
      ],
      "metadata": {
        "id": "iBpeuPRZjIDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Definisi Model PyTorch\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Inisialisasi model\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_classes = len(np.unique(y))\n",
        "model_torch = MLPModel(input_size, hidden_size, num_classes)\n",
        "\n",
        "\n",
        "# Definisi loss function dan optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_torch.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Early stopping manual PyTorch\n",
        "early_stop_patience = 5\n",
        "best_loss = float(\"inf\")\n",
        "patience = 0"
      ],
      "metadata": {
        "id": "ljOyIR4MZJJn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training model PyTorch"
      ],
      "metadata": {
        "id": "R6nl02EWigwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_torch(X_train_torch)\n",
        "    loss = criterion(outputs, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validasi\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model_torch(X_test_torch)\n",
        "        val_loss = criterion(val_outputs, y_test_torch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss.item() < best_loss:\n",
        "        best_loss = val_loss.item()\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "    if patience >= early_stop_patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfyNXwmpZgze",
        "outputId": "b913c4a2-c3f5-495b-f691-f362ada65b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.8829834461212158, Val Loss: 1.8428910970687866\n",
            "Epoch 2, Loss: 1.8428775072097778, Val Loss: 1.8040293455123901\n",
            "Epoch 3, Loss: 1.8039824962615967, Val Loss: 1.7663599252700806\n",
            "Epoch 4, Loss: 1.7662723064422607, Val Loss: 1.7298469543457031\n",
            "Epoch 5, Loss: 1.7297084331512451, Val Loss: 1.6944513320922852\n",
            "Epoch 6, Loss: 1.694251537322998, Val Loss: 1.660136103630066\n",
            "Epoch 7, Loss: 1.6598676443099976, Val Loss: 1.6268675327301025\n",
            "Epoch 8, Loss: 1.6265250444412231, Val Loss: 1.5946124792099\n",
            "Epoch 9, Loss: 1.594191074371338, Val Loss: 1.5633388757705688\n",
            "Epoch 10, Loss: 1.5628360509872437, Val Loss: 1.5330162048339844\n",
            "Epoch 11, Loss: 1.532429814338684, Val Loss: 1.5036154985427856\n",
            "Epoch 12, Loss: 1.5029443502426147, Val Loss: 1.4751086235046387\n",
            "Epoch 13, Loss: 1.4743516445159912, Val Loss: 1.4474668502807617\n",
            "Epoch 14, Loss: 1.4466248750686646, Val Loss: 1.4206658601760864\n",
            "Epoch 15, Loss: 1.419738531112671, Val Loss: 1.3946832418441772\n",
            "Epoch 16, Loss: 1.3936718702316284, Val Loss: 1.3694980144500732\n",
            "Epoch 17, Loss: 1.3684042692184448, Val Loss: 1.3450911045074463\n",
            "Epoch 18, Loss: 1.343917965888977, Val Loss: 1.3214455842971802\n",
            "Epoch 19, Loss: 1.3201977014541626, Val Loss: 1.2985471487045288\n",
            "Epoch 20, Loss: 1.297228217124939, Val Loss: 1.2763822078704834\n",
            "Epoch 21, Loss: 1.2749963998794556, Val Loss: 1.2549388408660889\n",
            "Epoch 22, Loss: 1.2534890174865723, Val Loss: 1.234205961227417\n",
            "Epoch 23, Loss: 1.2326961755752563, Val Loss: 1.2141716480255127\n",
            "Epoch 24, Loss: 1.2126061916351318, Val Loss: 1.1948250532150269\n",
            "Epoch 25, Loss: 1.1932085752487183, Val Loss: 1.1761553287506104\n",
            "Epoch 26, Loss: 1.174493670463562, Val Loss: 1.158152461051941\n",
            "Epoch 27, Loss: 1.156449794769287, Val Loss: 1.1408047676086426\n",
            "Epoch 28, Loss: 1.1390656232833862, Val Loss: 1.1241010427474976\n",
            "Epoch 29, Loss: 1.122329592704773, Val Loss: 1.1080293655395508\n",
            "Epoch 30, Loss: 1.1062297821044922, Val Loss: 1.0925770998001099\n",
            "Epoch 31, Loss: 1.090753436088562, Val Loss: 1.0777307748794556\n",
            "Epoch 32, Loss: 1.0758862495422363, Val Loss: 1.0634764432907104\n",
            "Epoch 33, Loss: 1.0616140365600586, Val Loss: 1.049798607826233\n",
            "Epoch 34, Loss: 1.0479204654693604, Val Loss: 1.0366804599761963\n",
            "Epoch 35, Loss: 1.034788966178894, Val Loss: 1.0241038799285889\n",
            "Epoch 36, Loss: 1.0222018957138062, Val Loss: 1.012052059173584\n",
            "Epoch 37, Loss: 1.0101418495178223, Val Loss: 1.0005072355270386\n",
            "Epoch 38, Loss: 0.998589813709259, Val Loss: 0.9894500970840454\n",
            "Epoch 39, Loss: 0.9875276684761047, Val Loss: 0.9788611531257629\n",
            "Epoch 40, Loss: 0.9769362211227417, Val Loss: 0.9687212109565735\n",
            "Epoch 41, Loss: 0.9667952060699463, Val Loss: 0.9590116739273071\n",
            "Epoch 42, Loss: 0.9570847749710083, Val Loss: 0.9497129917144775\n",
            "Epoch 43, Loss: 0.9477853178977966, Val Loss: 0.9408050179481506\n",
            "Epoch 44, Loss: 0.938877522945404, Val Loss: 0.9322683215141296\n",
            "Epoch 45, Loss: 0.930341899394989, Val Loss: 0.9240838289260864\n",
            "Epoch 46, Loss: 0.9221596717834473, Val Loss: 0.9162341356277466\n",
            "Epoch 47, Loss: 0.9143131375312805, Val Loss: 0.9087017178535461\n",
            "Epoch 48, Loss: 0.9067839980125427, Val Loss: 0.9014700651168823\n",
            "Epoch 49, Loss: 0.8995550870895386, Val Loss: 0.8945220708847046\n",
            "Epoch 50, Loss: 0.8926107287406921, Val Loss: 0.8878448009490967\n",
            "Epoch 51, Loss: 0.8859365582466125, Val Loss: 0.8814252018928528\n",
            "Epoch 52, Loss: 0.8795194029808044, Val Loss: 0.875251293182373\n",
            "Epoch 53, Loss: 0.8733474016189575, Val Loss: 0.8693137168884277\n",
            "Epoch 54, Loss: 0.8674100637435913, Val Loss: 0.8636029958724976\n",
            "Epoch 55, Loss: 0.8616981506347656, Val Loss: 0.8581106662750244\n",
            "Epoch 56, Loss: 0.856203019618988, Val Loss: 0.8528289198875427\n",
            "Epoch 57, Loss: 0.8509163856506348, Val Loss: 0.8477522134780884\n",
            "Epoch 58, Loss: 0.8458317518234253, Val Loss: 0.8428745865821838\n",
            "Epoch 59, Loss: 0.8409433364868164, Val Loss: 0.838188886642456\n",
            "Epoch 60, Loss: 0.8362447619438171, Val Loss: 0.8336876630783081\n",
            "Epoch 61, Loss: 0.8317285776138306, Val Loss: 0.8293633460998535\n",
            "Epoch 62, Loss: 0.8273871541023254, Val Loss: 0.825209379196167\n",
            "Epoch 63, Loss: 0.8232136964797974, Val Loss: 0.8212178349494934\n",
            "Epoch 64, Loss: 0.8192012310028076, Val Loss: 0.817382276058197\n",
            "Epoch 65, Loss: 0.8153434991836548, Val Loss: 0.813696026802063\n",
            "Epoch 66, Loss: 0.8116346001625061, Val Loss: 0.8101528882980347\n",
            "Epoch 67, Loss: 0.8080698251724243, Val Loss: 0.8067464828491211\n",
            "Epoch 68, Loss: 0.8046451210975647, Val Loss: 0.8034704327583313\n",
            "Epoch 69, Loss: 0.8013522028923035, Val Loss: 0.8003179430961609\n",
            "Epoch 70, Loss: 0.7981824278831482, Val Loss: 0.7972822189331055\n",
            "Epoch 71, Loss: 0.7951289415359497, Val Loss: 0.7943560481071472\n",
            "Epoch 72, Loss: 0.7921845316886902, Val Loss: 0.7915329337120056\n",
            "Epoch 73, Loss: 0.7893427014350891, Val Loss: 0.7888060808181763\n",
            "Epoch 74, Loss: 0.7865970730781555, Val Loss: 0.7861695289611816\n",
            "Epoch 75, Loss: 0.783941924571991, Val Loss: 0.7836175560951233\n",
            "Epoch 76, Loss: 0.7813712358474731, Val Loss: 0.7811450362205505\n",
            "Epoch 77, Loss: 0.7788795828819275, Val Loss: 0.7787474393844604\n",
            "Epoch 78, Loss: 0.7764624953269958, Val Loss: 0.7764202952384949\n",
            "Epoch 79, Loss: 0.7741156220436096, Val Loss: 0.7741602659225464\n",
            "Epoch 80, Loss: 0.771835207939148, Val Loss: 0.771963894367218\n",
            "Epoch 81, Loss: 0.7696177959442139, Val Loss: 0.769827663898468\n",
            "Epoch 82, Loss: 0.7674605250358582, Val Loss: 0.7677482962608337\n",
            "Epoch 83, Loss: 0.7653600573539734, Val Loss: 0.7657232284545898\n",
            "Epoch 84, Loss: 0.7633136510848999, Val Loss: 0.763748824596405\n",
            "Epoch 85, Loss: 0.7613182663917542, Val Loss: 0.7618218660354614\n",
            "Epoch 86, Loss: 0.7593713998794556, Val Loss: 0.7599394917488098\n",
            "Epoch 87, Loss: 0.7574704885482788, Val Loss: 0.7580994367599487\n",
            "Epoch 88, Loss: 0.7556126117706299, Val Loss: 0.7562992572784424\n",
            "Epoch 89, Loss: 0.7537955641746521, Val Loss: 0.754536509513855\n",
            "Epoch 90, Loss: 0.7520173788070679, Val Loss: 0.7528107762336731\n",
            "Epoch 91, Loss: 0.7502763867378235, Val Loss: 0.7511197924613953\n",
            "Epoch 92, Loss: 0.7485712766647339, Val Loss: 0.7494621872901917\n",
            "Epoch 93, Loss: 0.7469003796577454, Val Loss: 0.747837245464325\n",
            "Epoch 94, Loss: 0.7452629208564758, Val Loss: 0.7462441325187683\n",
            "Epoch 95, Loss: 0.7436572909355164, Val Loss: 0.7446826100349426\n",
            "Epoch 96, Loss: 0.7420832514762878, Val Loss: 0.7431512475013733\n",
            "Epoch 97, Loss: 0.74053955078125, Val Loss: 0.7416495084762573\n",
            "Epoch 98, Loss: 0.7390249967575073, Val Loss: 0.7401763200759888\n",
            "Epoch 99, Loss: 0.7375388145446777, Val Loss: 0.7387304306030273\n",
            "Epoch 100, Loss: 0.736079752445221, Val Loss: 0.737311065196991\n",
            "Epoch 101, Loss: 0.7346464991569519, Val Loss: 0.7359163761138916\n",
            "Epoch 102, Loss: 0.7332377433776855, Val Loss: 0.7345452904701233\n",
            "Epoch 103, Loss: 0.7318522930145264, Val Loss: 0.7331967949867249\n",
            "Epoch 104, Loss: 0.7304885983467102, Val Loss: 0.7318693995475769\n",
            "Epoch 105, Loss: 0.729146420955658, Val Loss: 0.7305624485015869\n",
            "Epoch 106, Loss: 0.7278250455856323, Val Loss: 0.7292752265930176\n",
            "Epoch 107, Loss: 0.7265233397483826, Val Loss: 0.7280069589614868\n",
            "Epoch 108, Loss: 0.7252407670021057, Val Loss: 0.726757287979126\n",
            "Epoch 109, Loss: 0.7239764928817749, Val Loss: 0.7255250215530396\n",
            "Epoch 110, Loss: 0.7227302193641663, Val Loss: 0.7243101596832275\n",
            "Epoch 111, Loss: 0.7215015888214111, Val Loss: 0.7231130003929138\n",
            "Epoch 112, Loss: 0.7202910780906677, Val Loss: 0.7219328284263611\n",
            "Epoch 113, Loss: 0.7190974354743958, Val Loss: 0.72076815366745\n",
            "Epoch 114, Loss: 0.7179197669029236, Val Loss: 0.7196187376976013\n",
            "Epoch 115, Loss: 0.7167575359344482, Val Loss: 0.7184843420982361\n",
            "Epoch 116, Loss: 0.7156104445457458, Val Loss: 0.7173649072647095\n",
            "Epoch 117, Loss: 0.7144784331321716, Val Loss: 0.7162597179412842\n",
            "Epoch 118, Loss: 0.7133610248565674, Val Loss: 0.7151683568954468\n",
            "Epoch 119, Loss: 0.7122575640678406, Val Loss: 0.71409010887146\n",
            "Epoch 120, Loss: 0.7111679315567017, Val Loss: 0.7130250334739685\n",
            "Epoch 121, Loss: 0.7100918889045715, Val Loss: 0.711972713470459\n",
            "Epoch 122, Loss: 0.709028959274292, Val Loss: 0.7109329700469971\n",
            "Epoch 123, Loss: 0.7079789638519287, Val Loss: 0.7099054455757141\n",
            "Epoch 124, Loss: 0.706941545009613, Val Loss: 0.7088901400566101\n",
            "Epoch 125, Loss: 0.705916702747345, Val Loss: 0.7078866958618164\n",
            "Epoch 126, Loss: 0.7049038410186768, Val Loss: 0.7068955302238464\n",
            "Epoch 127, Loss: 0.7039027214050293, Val Loss: 0.7059157490730286\n",
            "Epoch 128, Loss: 0.7029133439064026, Val Loss: 0.7049476504325867\n",
            "Epoch 129, Loss: 0.701935350894928, Val Loss: 0.7039904594421387\n",
            "Epoch 130, Loss: 0.7009687423706055, Val Loss: 0.7030442357063293\n",
            "Epoch 131, Loss: 0.700012743473053, Val Loss: 0.7021087408065796\n",
            "Epoch 132, Loss: 0.6990674138069153, Val Loss: 0.7011836171150208\n",
            "Epoch 133, Loss: 0.6981327533721924, Val Loss: 0.7002686858177185\n",
            "Epoch 134, Loss: 0.6972082853317261, Val Loss: 0.699363648891449\n",
            "Epoch 135, Loss: 0.6962942481040955, Val Loss: 0.6984682083129883\n",
            "Epoch 136, Loss: 0.6953900456428528, Val Loss: 0.6975822448730469\n",
            "Epoch 137, Loss: 0.694495439529419, Val Loss: 0.6967050433158875\n",
            "Epoch 138, Loss: 0.6936102509498596, Val Loss: 0.6958369612693787\n",
            "Epoch 139, Loss: 0.6927345991134644, Val Loss: 0.694977343082428\n",
            "Epoch 140, Loss: 0.6918681263923645, Val Loss: 0.6941261887550354\n",
            "Epoch 141, Loss: 0.6910107731819153, Val Loss: 0.69328373670578\n",
            "Epoch 142, Loss: 0.6901624798774719, Val Loss: 0.6924498081207275\n",
            "Epoch 143, Loss: 0.6893230676651001, Val Loss: 0.6916241645812988\n",
            "Epoch 144, Loss: 0.688492476940155, Val Loss: 0.6908069252967834\n",
            "Epoch 145, Loss: 0.687670886516571, Val Loss: 0.6899988055229187\n",
            "Epoch 146, Loss: 0.6868587136268616, Val Loss: 0.6891990900039673\n",
            "Epoch 147, Loss: 0.6860557198524475, Val Loss: 0.6884078979492188\n",
            "Epoch 148, Loss: 0.6852613091468811, Val Loss: 0.6876246333122253\n",
            "Epoch 149, Loss: 0.684475302696228, Val Loss: 0.6868493556976318\n",
            "Epoch 150, Loss: 0.6836976408958435, Val Loss: 0.6860823035240173\n",
            "Epoch 151, Loss: 0.6829281449317932, Val Loss: 0.6853232383728027\n",
            "Epoch 152, Loss: 0.6821668148040771, Val Loss: 0.6845716238021851\n",
            "Epoch 153, Loss: 0.6814132928848267, Val Loss: 0.6838268637657166\n",
            "Epoch 154, Loss: 0.6806671619415283, Val Loss: 0.6830892562866211\n",
            "Epoch 155, Loss: 0.6799288392066956, Val Loss: 0.6823586821556091\n",
            "Epoch 156, Loss: 0.6791979074478149, Val Loss: 0.6816351413726807\n",
            "Epoch 157, Loss: 0.6784743070602417, Val Loss: 0.6809184551239014\n",
            "Epoch 158, Loss: 0.6777577996253967, Val Loss: 0.6802080869674683\n",
            "Epoch 159, Loss: 0.6770479083061218, Val Loss: 0.6795037984848022\n",
            "Epoch 160, Loss: 0.6763445734977722, Val Loss: 0.6788062453269958\n",
            "Epoch 161, Loss: 0.6756477355957031, Val Loss: 0.6781151294708252\n",
            "Epoch 162, Loss: 0.6749575734138489, Val Loss: 0.6774306893348694\n",
            "Epoch 163, Loss: 0.674274206161499, Val Loss: 0.6767529249191284\n",
            "Epoch 164, Loss: 0.6735974550247192, Val Loss: 0.676081657409668\n",
            "Epoch 165, Loss: 0.6729274392127991, Val Loss: 0.6754169464111328\n",
            "Epoch 166, Loss: 0.6722636818885803, Val Loss: 0.6747589707374573\n",
            "Epoch 167, Loss: 0.6716069579124451, Val Loss: 0.6741076111793518\n",
            "Epoch 168, Loss: 0.670956552028656, Val Loss: 0.6734626293182373\n",
            "Epoch 169, Loss: 0.6703122854232788, Val Loss: 0.6728235483169556\n",
            "Epoch 170, Loss: 0.6696741580963135, Val Loss: 0.6721905469894409\n",
            "Epoch 171, Loss: 0.6690418720245361, Val Loss: 0.6715635061264038\n",
            "Epoch 172, Loss: 0.6684153079986572, Val Loss: 0.6709418892860413\n",
            "Epoch 173, Loss: 0.6677944660186768, Val Loss: 0.670325756072998\n",
            "Epoch 174, Loss: 0.6671794652938843, Val Loss: 0.669715404510498\n",
            "Epoch 175, Loss: 0.6665697693824768, Val Loss: 0.6691098213195801\n",
            "Epoch 176, Loss: 0.6659656763076782, Val Loss: 0.6685099005699158\n",
            "Epoch 177, Loss: 0.6653672456741333, Val Loss: 0.6679165363311768\n",
            "Epoch 178, Loss: 0.664775013923645, Val Loss: 0.6673287153244019\n",
            "Epoch 179, Loss: 0.6641883254051208, Val Loss: 0.6667462587356567\n",
            "Epoch 180, Loss: 0.663607120513916, Val Loss: 0.6661686897277832\n",
            "Epoch 181, Loss: 0.663031280040741, Val Loss: 0.6655960083007812\n",
            "Epoch 182, Loss: 0.6624608039855957, Val Loss: 0.6650283336639404\n",
            "Epoch 183, Loss: 0.6618952751159668, Val Loss: 0.6644653677940369\n",
            "Epoch 184, Loss: 0.6613348722457886, Val Loss: 0.6639074683189392\n",
            "Epoch 185, Loss: 0.6607796549797058, Val Loss: 0.6633546948432922\n",
            "Epoch 186, Loss: 0.660229504108429, Val Loss: 0.6628068089485168\n",
            "Epoch 187, Loss: 0.6596844792366028, Val Loss: 0.6622639298439026\n",
            "Epoch 188, Loss: 0.6591442823410034, Val Loss: 0.6617259979248047\n",
            "Epoch 189, Loss: 0.6586088538169861, Val Loss: 0.6611928343772888\n",
            "Epoch 190, Loss: 0.6580782532691956, Val Loss: 0.6606642603874207\n",
            "Epoch 191, Loss: 0.6575523018836975, Val Loss: 0.660140335559845\n",
            "Epoch 192, Loss: 0.6570312976837158, Val Loss: 0.6596209406852722\n",
            "Epoch 193, Loss: 0.6565147638320923, Val Loss: 0.6591058969497681\n",
            "Epoch 194, Loss: 0.656002938747406, Val Loss: 0.658595085144043\n",
            "Epoch 195, Loss: 0.6554955244064331, Val Loss: 0.6580884456634521\n",
            "Epoch 196, Loss: 0.6549925804138184, Val Loss: 0.6575862169265747\n",
            "Epoch 197, Loss: 0.6544939875602722, Val Loss: 0.6570882201194763\n",
            "Epoch 198, Loss: 0.6539996266365051, Val Loss: 0.6565945148468018\n",
            "Epoch 199, Loss: 0.6535096764564514, Val Loss: 0.6561046838760376\n",
            "Epoch 200, Loss: 0.6530237197875977, Val Loss: 0.6556189060211182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Model PyTorch"
      ],
      "metadata": {
        "id": "a9C32blHiXGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_torch = torch.argmax(model_torch(X_test_torch), axis=1).numpy()\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred_torch))\n",
        "print(\"Presisi:\", precision_score(y_test, y_pred_torch, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_torch, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_torch, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9knS5RQayPu",
        "outputId": "bb6ea695-b6c1-4a99-f226-6bae4456ca3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.7291636188394448\n",
            "Presisi: 0.7247375533871977\n",
            "Recall: 0.7291636188394448\n",
            "F1 Score: 0.7134739203324851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model TensorFlow"
      ],
      "metadata": {
        "id": "iBL0ADR4iHr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi ke TensorFlow\n",
        "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "\n",
        "\n",
        "# Definisi Model TensorFlow\n",
        "model_tf = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(input_size,)),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Early stopping TensorFlow\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Training model TensorFlow\n",
        "model_tf.fit(X_train_tf, y_train_tf, epochs=30, validation_data=(X_test_tf, y_test_tf), callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMNSdvswbJRT",
        "outputId": "2cb935aa-d2bc-490e-d674-9315d6ea860f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.7362 - loss: 0.6261 - val_accuracy: 0.7843 - val_loss: 0.4994\n",
            "Epoch 2/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step - accuracy: 0.7884 - loss: 0.4906 - val_accuracy: 0.8020 - val_loss: 0.4672\n",
            "Epoch 3/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - accuracy: 0.8055 - loss: 0.4565 - val_accuracy: 0.8125 - val_loss: 0.4459\n",
            "Epoch 4/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8166 - loss: 0.4348 - val_accuracy: 0.8281 - val_loss: 0.4194\n",
            "Epoch 5/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - accuracy: 0.8232 - loss: 0.4207 - val_accuracy: 0.8276 - val_loss: 0.4163\n",
            "Epoch 6/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4089 - val_accuracy: 0.8316 - val_loss: 0.4015\n",
            "Epoch 7/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - accuracy: 0.8326 - loss: 0.4010 - val_accuracy: 0.8373 - val_loss: 0.3903\n",
            "Epoch 8/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8372 - loss: 0.3929 - val_accuracy: 0.8413 - val_loss: 0.3906\n",
            "Epoch 9/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8399 - loss: 0.3860 - val_accuracy: 0.8374 - val_loss: 0.3913\n",
            "Epoch 10/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3ms/step - accuracy: 0.8430 - loss: 0.3804 - val_accuracy: 0.8415 - val_loss: 0.3826\n",
            "Epoch 11/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - accuracy: 0.8443 - loss: 0.3781 - val_accuracy: 0.8374 - val_loss: 0.3916\n",
            "Epoch 12/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - accuracy: 0.8459 - loss: 0.3745 - val_accuracy: 0.8455 - val_loss: 0.3754\n",
            "Epoch 13/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8482 - loss: 0.3703 - val_accuracy: 0.8474 - val_loss: 0.3682\n",
            "Epoch 14/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8489 - loss: 0.3679 - val_accuracy: 0.8493 - val_loss: 0.3678\n",
            "Epoch 15/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - accuracy: 0.8501 - loss: 0.3661 - val_accuracy: 0.8422 - val_loss: 0.3836\n",
            "Epoch 16/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3ms/step - accuracy: 0.8510 - loss: 0.3627 - val_accuracy: 0.8483 - val_loss: 0.3711\n",
            "Epoch 17/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.3600 - val_accuracy: 0.8439 - val_loss: 0.3823\n",
            "Epoch 18/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.3580 - val_accuracy: 0.8510 - val_loss: 0.3680\n",
            "Epoch 19/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3ms/step - accuracy: 0.8541 - loss: 0.3565 - val_accuracy: 0.8501 - val_loss: 0.3636\n",
            "Epoch 20/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8550 - loss: 0.3556 - val_accuracy: 0.8571 - val_loss: 0.3543\n",
            "Epoch 21/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 3ms/step - accuracy: 0.8565 - loss: 0.3526 - val_accuracy: 0.8523 - val_loss: 0.3648\n",
            "Epoch 22/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8564 - loss: 0.3511 - val_accuracy: 0.8557 - val_loss: 0.3556\n",
            "Epoch 23/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step - accuracy: 0.8573 - loss: 0.3501 - val_accuracy: 0.8597 - val_loss: 0.3515\n",
            "Epoch 24/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.8574 - loss: 0.3475 - val_accuracy: 0.8612 - val_loss: 0.3478\n",
            "Epoch 25/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.8580 - loss: 0.3461 - val_accuracy: 0.8538 - val_loss: 0.3555\n",
            "Epoch 26/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.8582 - loss: 0.3459 - val_accuracy: 0.8601 - val_loss: 0.3464\n",
            "Epoch 27/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3ms/step - accuracy: 0.8603 - loss: 0.3431 - val_accuracy: 0.8590 - val_loss: 0.3521\n",
            "Epoch 28/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2ms/step - accuracy: 0.8603 - loss: 0.3436 - val_accuracy: 0.8514 - val_loss: 0.3653\n",
            "Epoch 29/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.8623 - loss: 0.3391 - val_accuracy: 0.8618 - val_loss: 0.3401\n",
            "Epoch 30/30\n",
            "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.8624 - loss: 0.3397 - val_accuracy: 0.8631 - val_loss: 0.3408\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79944d4f2590>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Hasil TensorFlow"
      ],
      "metadata": {
        "id": "kjCk-G79iBdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_tf = np.argmax(model_tf.predict(X_test_tf), axis=1)\n",
        "print(\"\\nEvaluasi TensorFlow:\")\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred_tf))\n",
        "print(\"Presisi:\", precision_score(y_test, y_pred_tf, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_tf, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_tf, average='weighted'))\n",
        "\n",
        "try:\n",
        "    print(\"ROC AUC Score:\", roc_auc_score(y_test, model_tf.predict(X_test_tf), multi_class='ovr'))\n",
        "except ValueError as e:\n",
        "    print(f\"ROC AUC Score tidak dapat dihitung: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRo90oGBh0cE",
        "outputId": "a339857e-80f1-4ac2-f0c7-56d329827e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3632/3632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n",
            "\n",
            "Evaluasi TensorFlow:\n",
            "Akurasi: 0.8618107966231509\n",
            "Presisi: 0.8621953112289097\n",
            "Recall: 0.8618107966231509\n",
            "F1 Score: 0.8618037765055826\n",
            "\u001b[1m3632/3632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
            "ROC AUC Score: 0.9838398807623617\n"
          ]
        }
      ]
    }
  ]
}
